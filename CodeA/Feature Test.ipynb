{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "018a557d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shikh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shikh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cdd0941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "postive_data_path = \"../CodeA/data/data/pos/\"\n",
    "negative_data_path =\"../CodeA/data/data/neg/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3db513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_files = os.listdir(postive_data_path)\n",
    "for f in range(len(pos_files)):\n",
    "    pos_files[f] = postive_data_path + pos_files[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "219017eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_files = os.listdir(negative_data_path)\n",
    "for f in range(len(neg_files)):\n",
    "    neg_files[f] = negative_data_path + neg_files[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b66c32e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_labels = [1]*len(pos_files)\n",
    "neg_labels = [0]*len(neg_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "267d2901",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"file_names\",\"labels\"])\n",
    "df[\"file_names\"] = pos_files + neg_files\n",
    "df[\"labels\"] = pos_labels + neg_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9385dcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to get the contents in those .txt files.\n",
    "\n",
    "def get_data(df):\n",
    "    all_txt = []\n",
    "    for i,j in df.iterrows(): #https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iterrows.html\n",
    "        with open(j[\"file_names\"],'r', encoding='utf-8') as f: #Asked GPT since there was an error loading the data \n",
    "            txt = f.read()\n",
    "            all_txt.append(txt)\n",
    "#         print(j)\n",
    "    df[\"reviews\"] = all_txt\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e6c31a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "733ccc85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_names</th>\n",
       "      <th>labels</th>\n",
       "      <th>reviews</th>\n",
       "      <th>tokenized_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../CodeA/data/data/pos/10000_8.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Homelessness (or Houselessness as George Carli...</td>\n",
       "      <td>[homelessness, (, or, houselessness, as, georg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../CodeA/data/data/pos/10008_7.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>You know, Robin Williams, God bless him, is co...</td>\n",
       "      <td>[you, know, ,, robin, williams, ,, god, bless,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../CodeA/data/data/pos/10013_7.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Like one of the previous commenters said, this...</td>\n",
       "      <td>[like, one, of, the, previous, commenters, sai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../CodeA/data/data/pos/10019_8.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>When it comes to movies I can be pretty picky,...</td>\n",
       "      <td>[when, it, comes, to, movies, i, can, be, pret...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../CodeA/data/data/pos/10020_8.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>The legendary Boris Karloff ended his illustri...</td>\n",
       "      <td>[the, legendary, boris, karloff, ended, his, i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           file_names  labels  \\\n",
       "0  ../CodeA/data/data/pos/10000_8.txt       1   \n",
       "1  ../CodeA/data/data/pos/10008_7.txt       1   \n",
       "2  ../CodeA/data/data/pos/10013_7.txt       1   \n",
       "3  ../CodeA/data/data/pos/10019_8.txt       1   \n",
       "4  ../CodeA/data/data/pos/10020_8.txt       1   \n",
       "\n",
       "                                             reviews  \\\n",
       "0  Homelessness (or Houselessness as George Carli...   \n",
       "1  You know, Robin Williams, God bless him, is co...   \n",
       "2  Like one of the previous commenters said, this...   \n",
       "3  When it comes to movies I can be pretty picky,...   \n",
       "4  The legendary Boris Karloff ended his illustri...   \n",
       "\n",
       "                                    tokenized_review  \n",
       "0  [homelessness, (, or, houselessness, as, georg...  \n",
       "1  [you, know, ,, robin, williams, ,, god, bless,...  \n",
       "2  [like, one, of, the, previous, commenters, sai...  \n",
       "3  [when, it, comes, to, movies, i, can, be, pret...  \n",
       "4  [the, legendary, boris, karloff, ended, his, i...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenization - Data \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized = []\n",
    "\n",
    "for i in df.index:\n",
    "    tokenized.append(word_tokenize(df['reviews'][i].lower()))\n",
    "    #tokenized.sort()\n",
    "    \n",
    "df.insert(3,'tokenized_review', tokenized,True) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a71a715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b822c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193a9f79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf60a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c974b7c3",
   "metadata": {},
   "source": [
    "**Stemming, bigrams and TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc807db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import LancasterStemmer\n",
    "import nltk\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "# Function to calculate TF-IDF with n-grams and filter based on frequency threshold\n",
    "def calculate_tfidf_ngrams_with_threshold(documents, n, threshold):\n",
    "    # Apply stemming\n",
    "    st = LancasterStemmer()\n",
    "    stemmed_documents = [[' '.join([st.stem(word) for word in words])] for words in documents]\n",
    "\n",
    "    # Generate n-grams and calculate frequency\n",
    "    all_bigrams = []\n",
    "    for doc in stemmed_documents:\n",
    "        #Splitting terms in the document to words(bigrams)\n",
    "        terms = list(ngrams(doc[0].split(), n))\n",
    "        all_bigrams.extend(terms)\n",
    "\n",
    "    # Calculate the frequency of all bigrams\n",
    "    frequency_word_bigrams = nltk.FreqDist(all_bigrams)\n",
    "\n",
    "    # Filter bigrams based on the threshold\n",
    "    filtered_bigrams = {bigram: freq for bigram, freq in frequency_word_bigrams.items() if freq >= threshold}\n",
    "\n",
    "    # Create a vocabulary\n",
    "    all_terms = set(filtered_bigrams.keys())\n",
    "    vocabulary = sorted(list(all_terms))\n",
    "\n",
    "    # Calculate TF\n",
    "    #https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html\n",
    "    tf_matrix = pd.DataFrame(0, index=df.index, columns=vocabulary, dtype=float)\n",
    "    #Iterating through index and documents \n",
    "    for i, doc in enumerate(stemmed_documents): #https://realpython.com/python-enumerate/\n",
    "        terms = list(ngrams(doc[0].split(), n))\n",
    "        for term in terms:\n",
    "            #counting the frequency of each term in each document.\n",
    "            #Incrementing the tf matrix by 1 if the term is in the vocabulary\n",
    "            if term in vocabulary:\n",
    "                tf_matrix.at[i, term] += 1\n",
    "\n",
    "    # Calculate IDF\n",
    "    #https://pandas.pydata.org/docs/reference/api/pandas.Series.html\n",
    "    idf_vector = pd.Series(0, index=vocabulary, dtype=float)\n",
    "    N = len(stemmed_documents)\n",
    "    for term in vocabulary:\n",
    "        #counting the number of documents in which the bigram appears \n",
    "        df_term = sum([1 for doc in stemmed_documents if term in list(ngrams(doc[0].split(), n))])\n",
    "        #Calculating the IDF value of each bigram\n",
    "        idf_vector.at[term] = math.log(N / (1 + df_term), 10)\n",
    "\n",
    "    # Calculate TF-IDF\n",
    "    tfidf_matrix = tf_matrix * idf_vector\n",
    "\n",
    "    return tfidf_matrix, vocabulary\n",
    "\n",
    "# Calculate TF-IDF matrix with bigrams and filter based on frequency threshold\n",
    "tfidf_matrix, vocabulary = calculate_tfidf_ngrams_with_threshold(df['tokenized_review'], n=2, threshold=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b2eb780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, developement  and test sets with Feature 1 - Stemming, removed stopwords and punctuation\n",
    "# and Applied TF-IDF\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(tfidf_matrix, df.labels, test_size=0.2, random_state=42)\n",
    "X_train1, X_dev1, y_train1, y_dev1 = train_test_split(X_train1, y_train1,test_size=0.2, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afa90641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.765625\n"
     ]
    }
   ],
   "source": [
    "# Create and train a machine learning model (e.g., Naive Bayes) - for Feature 1\n",
    "model_1 = MultinomialNB()\n",
    "model_1.fit(X_train1, y_train1)\n",
    "model_1.predict(X_dev1)\n",
    "# Evaluate the model's performance on the dev data\n",
    "accuracy1 = model_1.score(X_dev1, y_dev1)\n",
    "print(accuracy1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0259a9",
   "metadata": {},
   "source": [
    "**Lemmatization, removed stopwords and punctuations, bigrams and TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac057f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "stoplist = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Function to calculate TF-IDF with n-grams and filter based on frequency threshold\n",
    "def calculate_tfidf_ngrams_with_threshold(documents, n, threshold):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_documents = [[' '.join([lemmatizer.lemmatize(word) for word in words if word not in stoplist and word not in string.punctuation])] for words in documents]\n",
    "\n",
    "    # Generate n-grams and calculate frequency\n",
    "    all_bigrams = []\n",
    "    for doc in lemmatized_documents:\n",
    "        terms = list(ngrams(doc[0].split(), n))\n",
    "        all_bigrams.extend(terms)\n",
    "\n",
    "    # Calculate the frequency of all bigrams\n",
    "    frequency_word_bigrams = nltk.FreqDist(all_bigrams)\n",
    "\n",
    "    # Filter bigrams based on the threshold\n",
    "    filtered_bigrams = {bigram: freq for bigram, freq in frequency_word_bigrams.items() if freq >= threshold}\n",
    "\n",
    "    # Create a vocabulary\n",
    "    all_terms = set(filtered_bigrams.keys())\n",
    "    vocabulary_2 = sorted(list(all_terms))\n",
    "\n",
    "    # Calculate TF\n",
    "    tf_matrix = pd.DataFrame(0, index=range(len(lemmatized_documents)), columns=vocabulary_2, dtype=float)\n",
    "    for i, doc in enumerate(lemmatized_documents):\n",
    "        terms = list(ngrams(doc[0].split(), n))\n",
    "        for term in terms:\n",
    "            if term in vocabulary_2:\n",
    "                tf_matrix.at[i, term] += 1\n",
    "\n",
    "    # Calculate IDF\n",
    "    idf_vector = pd.Series(0, index=vocabulary_2, dtype=float)\n",
    "    N = len(lemmatized_documents)\n",
    "    for term in vocabulary_2:\n",
    "        df_term = sum([1 for doc in lemmatized_documents if term in list(ngrams(doc[0].split(), n))])\n",
    "        idf_vector.at[term] = math.log(N / (1 + df_term), 10)\n",
    "\n",
    "    # Calculate TF-IDF\n",
    "    tfidf_matrix_2 = tf_matrix * idf_vector\n",
    "\n",
    "    return tfidf_matrix_2, vocabulary_2\n",
    "\n",
    "# Calculate TF-IDF matrix with bigrams and filter based on frequency threshold\n",
    "tfidf_matrix_2, vocabulary_2 = calculate_tfidf_ngrams_with_threshold(df['tokenized_review'], n=2, threshold=100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96b0b40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89594c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, developement  and test sets with Feature 1 - Stemming, removed stopwords and punctuation\n",
    "# and Applied TF-IDF\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(tfidf_matrix_2, df.labels, test_size=0.2, random_state=42)\n",
    "X_train2, X_dev2, y_train2, y_dev2 = train_test_split(X_train2, y_train2,test_size=0.2, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cac2524f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.534375\n"
     ]
    }
   ],
   "source": [
    "# Create and train a machine learning model (e.g., Naive Bayes) - for Feature 1\n",
    "model_2 = MultinomialNB()\n",
    "model_2.fit(X_train2, y_train2)\n",
    "model_2.predict(X_dev2)\n",
    "# Evaluate the model's performance on the dev data\n",
    "accuracy2 = model_2.score(X_dev2, y_dev2)\n",
    "print(accuracy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3869f76",
   "metadata": {},
   "source": [
    "**Stemming, removed stopwords and punctuations, bigrams and TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12b858ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import LancasterStemmer\n",
    "import nltk\n",
    "import math\n",
    "\n",
    "stoplist = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Function to calculate TF-IDF with n-grams and filter based on frequency threshold\n",
    "def calculate_tfidf_ngrams_with_threshold(documents, n, threshold):\n",
    "    # Apply stemming\n",
    "    st = LancasterStemmer()\n",
    "    stemmed_documents_2 = [[' '.join([st.stem(word) for word in words if word not in stoplist and word not in string.punctuation])] for words in documents]\n",
    "\n",
    "\n",
    "    # Generate n-grams and calculate frequency\n",
    "    all_bigrams = []\n",
    "    for doc in stemmed_documents_2:\n",
    "        terms = list(ngrams(doc[0].split(), n))\n",
    "        all_bigrams.extend(terms)\n",
    "\n",
    "    # Calculate the frequency of all bigrams\n",
    "    frequency_word_bigrams = nltk.FreqDist(all_bigrams)\n",
    "\n",
    "    # Filter bigrams based on the threshold\n",
    "    filtered_bigrams = {bigram: freq for bigram, freq in frequency_word_bigrams.items() if freq >= threshold}\n",
    "\n",
    "    # Create a vocabulary\n",
    "    all_terms = set(filtered_bigrams.keys())\n",
    "    vocabulary_3 = sorted(list(all_terms))\n",
    "\n",
    "    # Calculate TF\n",
    "    tf_matrix = pd.DataFrame(0, index=df.index, columns=vocabulary_3, dtype=float)\n",
    "    for i, doc in enumerate(stemmed_documents_2):\n",
    "        terms = list(ngrams(doc[0].split(), n))\n",
    "        for term in terms:\n",
    "            if term in vocabulary_3:\n",
    "                tf_matrix.at[i, term] += 1\n",
    "\n",
    "    # Calculate IDF\n",
    "    idf_vector = pd.Series(0, index=vocabulary_3, dtype=float)\n",
    "    N = len(stemmed_documents_2)\n",
    "    for term in vocabulary_3:\n",
    "        df_term = sum([1 for doc in stemmed_documents_2 if term in list(ngrams(doc[0].split(), n))])\n",
    "        idf_vector.at[term] = math.log(N / (1 + df_term), 10)\n",
    "\n",
    "    # Calculate TF-IDF\n",
    "    tfidf_matrix_3 = tf_matrix * idf_vector\n",
    "\n",
    "    return tfidf_matrix_3, vocabulary_3\n",
    "\n",
    "# Calculate TF-IDF matrix with bigrams and filter based on frequency threshold\n",
    "tfidf_matrix_3, vocabulary_3 = calculate_tfidf_ngrams_with_threshold(df['tokenized_review'], n=2, threshold=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1e7a3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, developement  and test sets with Feature 1 - Stemming, removed stopwords and punctuation\n",
    "# and Applied TF-IDF\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(tfidf_matrix_3, df.labels, test_size=0.2, random_state=42)\n",
    "X_train3, X_dev3, y_train3, y_dev3 = train_test_split(X_train3, y_train3,test_size=0.2, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5385d990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5421875\n"
     ]
    }
   ],
   "source": [
    "# Create and train a machine learning model (e.g., Naive Bayes) - for Feature 1\n",
    "model_3 = MultinomialNB()\n",
    "model_3.fit(X_train3, y_train3)\n",
    "model_3.predict(X_dev3)\n",
    "# Evaluate the model's performance on the dev data\n",
    "accuracy3 = model_3.score(X_dev3, y_dev3)\n",
    "print(accuracy3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de09847",
   "metadata": {},
   "source": [
    "**Stemming, removed stopwords and punctuations, unigrams and TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3935d60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import LancasterStemmer\n",
    "import math\n",
    "\n",
    "stoplist = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Function to calculate TF-IDF\n",
    "def calculate_tfidf(documents):\n",
    "    # Apply stemming\n",
    "    st = LancasterStemmer()\n",
    "    stemmed_documents_4 = [[' '.join([st.stem(word) for word in words if word not in stoplist and word not in string.punctuation])] for words in documents]\n",
    "\n",
    "\n",
    "    # Create a vocabulary\n",
    "    all_terms = set()\n",
    "    for doc in stemmed_documents_4:\n",
    "        all_terms.update(doc[0].split())\n",
    "    vocabulary_4 = sorted(list(all_terms))\n",
    "\n",
    "    # Calculate TF\n",
    "    tf_matrix = pd.DataFrame(0, index=df.index, columns=vocabulary_4, dtype=float)\n",
    "    for i, doc in enumerate(stemmed_documents_4):\n",
    "        for term in doc[0].split():\n",
    "            tf_matrix.at[i, term] += 1\n",
    "\n",
    "    # Calculate IDF\n",
    "    idf_vector = pd.Series(0, index=vocabulary_4, dtype=float)\n",
    "    N = len(stemmed_documents_4)\n",
    "    for term in vocabulary_4:\n",
    "        df_term = sum([1 for doc in stemmed_documents_4 if term in doc[0].split()])\n",
    "        idf_vector.at[term] = math.log(N / (1 + df_term), 10)\n",
    "\n",
    "    # Calculate TF-IDF\n",
    "    tfidf_matrix_4 = tf_matrix * idf_vector\n",
    "\n",
    "    return tfidf_matrix_4, vocabulary_4\n",
    "\n",
    "# Calculate TF-IDF matrix and get vocabulary\n",
    "tfidf_matrix_4, vocabulary_4 = calculate_tfidf(df['tokenized_review'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b77ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, developement  and test sets with Feature 1 - Stemming, removed stopwords and punctuation\n",
    "# and Applied TF-IDF\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(tfidf_matrix_4, df.labels, test_size=0.2, random_state=42)\n",
    "X_train4, X_dev4, y_train4, y_dev4 = train_test_split(X_train4, y_train4,test_size=0.2, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9370ec2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.784375\n"
     ]
    }
   ],
   "source": [
    "# Create and train a machine learning model (e.g., Naive Bayes) - for Feature 1\n",
    "model_4 = MultinomialNB()\n",
    "model_4.fit(X_train4, y_train4)\n",
    "model_4.predict(X_dev4)\n",
    "# Evaluate the model's performance on the dev data\n",
    "accuracy4 = model_4.score(X_dev4, y_dev4)\n",
    "print(accuracy4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a383c14",
   "metadata": {},
   "source": [
    "**Lemmatization, removed stopwords and punctuations, unigrams and TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f820c028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import LancasterStemmer\n",
    "import math\n",
    "\n",
    "stoplist = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Function to calculate TF-IDF\n",
    "def calculate_tfidf(documents):\n",
    "    # Apply stemming\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_documents_2 = [[' '.join([lemmatizer.lemmatize(word) for word in words if word not in stoplist and word not in string.punctuation])] for words in documents]\n",
    "\n",
    "\n",
    "    # Create a vocabulary\n",
    "    all_terms = set()\n",
    "    for doc in lemmatized_documents_2:\n",
    "        all_terms.update(doc[0].split())\n",
    "    vocabulary_5 = sorted(list(all_terms))\n",
    "\n",
    "    # Calculate TF\n",
    "    tf_matrix = pd.DataFrame(0, index=df.index, columns=vocabulary_5, dtype=float)\n",
    "    for i, doc in enumerate(lemmatized_documents_2):\n",
    "        for term in doc[0].split():\n",
    "            tf_matrix.at[i, term] += 1\n",
    "\n",
    "    # Calculate IDF\n",
    "    idf_vector = pd.Series(0, index=vocabulary_5, dtype=float)\n",
    "    N = len(lemmatized_documents_2)\n",
    "    for term in vocabulary_5:\n",
    "        df_term = sum([1 for doc in lemmatized_documents_2 if term in doc[0].split()])\n",
    "        idf_vector.at[term] = math.log(N / (1 + df_term), 10)\n",
    "\n",
    "    # Calculate TF-IDF\n",
    "    tfidf_matrix_5 = tf_matrix * idf_vector\n",
    "\n",
    "    return tfidf_matrix_5, vocabulary_5\n",
    "\n",
    "# Calculate TF-IDF matrix and get vocabulary\n",
    "tfidf_matrix_5, vocabulary_5 = calculate_tfidf(df['tokenized_review'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49ae1df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, developement  and test sets with Feature 1 - Stemming, removed stopwords and punctuation\n",
    "# and Applied TF-IDF\n",
    "X_train5, X_test5, y_train5, y_test5 = train_test_split(tfidf_matrix_5, df.labels, test_size=0.2, random_state=42)\n",
    "X_train5, X_dev5, y_train5, y_dev5 = train_test_split(X_train5, y_train5,test_size=0.2, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1ba523f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7859375\n"
     ]
    }
   ],
   "source": [
    "# Create and train a machine learning model (e.g., Naive Bayes) - for Feature 1\n",
    "model_5 = MultinomialNB()\n",
    "model_5.fit(X_train5, y_train5)\n",
    "model_5.predict(X_dev5)\n",
    "# Evaluate the model's performance on the dev data\n",
    "accuracy5 = model_5.score(X_dev5, y_dev5)\n",
    "print(accuracy5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06be667",
   "metadata": {},
   "source": [
    "**Stemming, removed stopwords and punctuations, trigrams and TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46410681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import LancasterStemmer\n",
    "import nltk\n",
    "import math\n",
    "\n",
    "stoplist = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Function to calculate TF-IDF with n-grams and filter based on frequency threshold\n",
    "def calculate_tfidf_ngrams_with_threshold(documents, n, threshold):\n",
    "    # Apply stemming\n",
    "    st = LancasterStemmer()\n",
    "    stemmed_documents_5 = [[' '.join([st.stem(word) for word in words if word not in stoplist and word not in string.punctuation])] for words in documents]\n",
    "\n",
    "\n",
    "    # Generate n-grams and calculate frequency\n",
    "    all_bigrams = []\n",
    "    for doc in stemmed_documents_5:\n",
    "        terms = list(ngrams(doc[0].split(), n))\n",
    "        all_bigrams.extend(terms)\n",
    "\n",
    "    # Calculate the frequency of all bigrams\n",
    "    frequency_word_bigrams = nltk.FreqDist(all_bigrams)\n",
    "\n",
    "    # Filter bigrams based on the threshold\n",
    "    filtered_bigrams = {bigram: freq for bigram, freq in frequency_word_bigrams.items() if freq >= threshold}\n",
    "\n",
    "    # Create a vocabulary\n",
    "    all_terms = set(filtered_bigrams.keys())\n",
    "    vocabulary_6 = sorted(list(all_terms))\n",
    "\n",
    "    # Calculate TF\n",
    "    tf_matrix = pd.DataFrame(0, index=df.index, columns=vocabulary_6, dtype=float)\n",
    "    for i, doc in enumerate(stemmed_documents_5):\n",
    "        terms = list(ngrams(doc[0].split(), n))\n",
    "        for term in terms:\n",
    "            if term in vocabulary_6:\n",
    "                tf_matrix.at[i, term] += 1\n",
    "\n",
    "    # Calculate IDF\n",
    "    idf_vector = pd.Series(0, index=vocabulary_6, dtype=float)\n",
    "    N = len(stemmed_documents_5)\n",
    "    for term in vocabulary_6:\n",
    "        df_term = sum([1 for doc in stemmed_documents_5 if term in list(ngrams(doc[0].split(), n))])\n",
    "        idf_vector.at[term] = math.log(N / (1 + df_term), 10)\n",
    "\n",
    "    # Calculate TF-IDF\n",
    "    tfidf_matrix_6 = tf_matrix * idf_vector\n",
    "\n",
    "    return tfidf_matrix_6, vocabulary_6\n",
    "\n",
    "# Calculate TF-IDF matrix with bigrams and filter based on frequency threshold\n",
    "tfidf_matrix_6, vocabulary_6 = calculate_tfidf_ngrams_with_threshold(df['tokenized_review'], n=3, threshold=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b60c692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, developement  and test sets with Feature 1 - Stemming, removed stopwords and punctuation\n",
    "# and Applied TF-IDF\n",
    "X_train6, X_test6, y_train6, y_test6 = train_test_split(tfidf_matrix_6, df.labels, test_size=0.2, random_state=42)\n",
    "X_train6, X_dev6, y_train6, y_dev6 = train_test_split(X_train6, y_train6,test_size=0.2, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "baae91b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.509375\n"
     ]
    }
   ],
   "source": [
    "# Create and train a machine learning model (e.g., Naive Bayes) - for Feature 1\n",
    "model_6 = MultinomialNB()\n",
    "model_6.fit(X_train6, y_train6)\n",
    "model_6.predict(X_dev6)\n",
    "# Evaluate the model's performance on the dev data\n",
    "accuracy6 = model_6.score(X_dev6, y_dev6)\n",
    "print(accuracy6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc1ae86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba3b190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11bc7a01",
   "metadata": {},
   "source": [
    "**Lemmatization, removed stopwords and punctuations, trigrams and TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e23d5179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "stoplist = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Function to calculate TF-IDF with n-grams and filter based on frequency threshold\n",
    "def calculate_tfidf_ngrams_with_threshold(documents, n, threshold):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_documents_3 = [[' '.join([lemmatizer.lemmatize(word) for word in words if word not in stoplist and word not in string.punctuation])] for words in documents]\n",
    "\n",
    "    # Generate n-grams and calculate frequency\n",
    "    all_bigrams = []\n",
    "    for doc in lemmatized_documents_3:\n",
    "        terms = list(ngrams(doc[0].split(), n))\n",
    "        all_bigrams.extend(terms)\n",
    "\n",
    "    # Calculate the frequency of all bigrams\n",
    "    frequency_word_bigrams = nltk.FreqDist(all_bigrams)\n",
    "\n",
    "    # Filter bigrams based on the threshold\n",
    "    filtered_bigrams = {bigram: freq for bigram, freq in frequency_word_bigrams.items() if freq >= threshold}\n",
    "\n",
    "    # Create a vocabulary\n",
    "    all_terms = set(filtered_bigrams.keys())\n",
    "    vocabulary_7 = sorted(list(all_terms))\n",
    "\n",
    "    # Calculate TF\n",
    "    tf_matrix = pd.DataFrame(0, index=range(len(lemmatized_documents_3)), columns=vocabulary_7, dtype=float)\n",
    "    for i, doc in enumerate(lemmatized_documents_3):\n",
    "        terms = list(ngrams(doc[0].split(), n))\n",
    "        for term in terms:\n",
    "            if term in vocabulary_7:\n",
    "                tf_matrix.at[i, term] += 1\n",
    "\n",
    "    # Calculate IDF\n",
    "    idf_vector = pd.Series(0, index=vocabulary_7, dtype=float)\n",
    "    N = len(lemmatized_documents_3)\n",
    "    for term in vocabulary_7:\n",
    "        df_term = sum([1 for doc in lemmatized_documents_3 if term in list(ngrams(doc[0].split(), n))])\n",
    "        idf_vector.at[term] = math.log(N / (1 + df_term), 10)\n",
    "\n",
    "    # Calculate TF-IDF\n",
    "    tfidf_matrix_7 = tf_matrix * idf_vector\n",
    "\n",
    "    return tfidf_matrix_7, vocabulary_7\n",
    "\n",
    "# Calculate TF-IDF matrix with bigrams and filter based on frequency threshold\n",
    "tfidf_matrix_7, vocabulary_7 = calculate_tfidf_ngrams_with_threshold(df['tokenized_review'], n=3, threshold=100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "172c364e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, developement  and test sets with Feature 1 - Stemming, removed stopwords and punctuation\n",
    "# and Applied TF-IDF\n",
    "X_train7, X_test7, y_train7, y_test7 = train_test_split(tfidf_matrix_7, df.labels, test_size=0.2, random_state=42)\n",
    "X_train7, X_dev7, y_train7, y_dev7 = train_test_split(X_train7, y_train7,test_size=0.2, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5f3f00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.50625\n"
     ]
    }
   ],
   "source": [
    "# Create and train a machine learning model (e.g., Naive Bayes) - for Feature 1\n",
    "model_7 = MultinomialNB()\n",
    "model_7.fit(X_train7, y_train7)\n",
    "model_7.predict(X_dev7)\n",
    "# Evaluate the model's performance on the dev data\n",
    "accuracy7 = model_7.score(X_dev7, y_dev7)\n",
    "print(accuracy7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdcd159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5f2d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35218c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05147473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9ead6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2655285",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
